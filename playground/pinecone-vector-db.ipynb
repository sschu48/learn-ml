{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pincone Embeddings Retriever\n",
    "\n",
    "Here we will develop a retriever that will perform a similarity search between our query and vectors within our vector database.\n",
    "\n",
    "Pinecone holds all of your embeddings in the \"cloud\". In order to find content that will add context to our query's answer, we will search this database for documents that are similar to the query. \n",
    "\n",
    "A similarity search is performed finding the closest vectors to the query vector within the n-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to initialize a connection with Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a Pinecone index that already has our embeddings. We will need  to create an connection to that database in order to query it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag-retriever-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 127}},\n",
       " 'total_vector_count': 127}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if index already exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec,\n",
    "    )\n",
    "#connect to index   \n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a connection to our index `rag-retriever-v1`\n",
    "It already has 127 vectors stored within it so we will use it to store our query embedding and perform similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cpu\") # choose the device to load model to\n",
    "\n",
    "# Notes: this will embed using local computing power. Learn more about the benefits (if any)\n",
    "# of computing in the cloud\n",
    "\n",
    "query = \"this is a single sentence\"\n",
    "res = embedding_model.encode(query)\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res.tolist()\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(vector=xq, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '122', 'score': 0.257955194, 'values': []},\n",
       "             {'id': '126', 'score': 0.189704508, 'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
